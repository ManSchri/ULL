{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all needed data\n",
    "\n",
    "def get_test_data(filename):\n",
    "    test_data = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line = line.split('\\t')\n",
    "            test_word = line[0]\n",
    "            sentence_id = line[1]\n",
    "            test_data.append([test_word, sentence_id])\n",
    "    return(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(filename):\n",
    "    with open(filename) as f:\n",
    "        candidate_dict = {}\n",
    "        for line in f:\n",
    "            line = line.split('::')\n",
    "            word = line[0]\n",
    "            candidates = line[1].split(';')\n",
    "            candidate_dict[word] = candidates\n",
    "        return candidate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# needs to be here to properly load the model - same as in Practical2_skipgram.ipynb\n",
    "class skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dimension):\n",
    "        super(skipgram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dimension = emb_dimension\n",
    "        # start with random embeddings\n",
    "#         self.W_embeddings = torch.randn((vocab_size, emb_dimension), requires_grad=True)\n",
    "#         self.C_embeddings = torch.randn((vocab_size, emb_dimension), requires_grad=True)\n",
    "        self.W_embeddings = nn.Embedding(vocab_size, emb_dimension)\n",
    "        self.C_embeddings = nn.Embedding(vocab_size, emb_dimension)\n",
    "        \n",
    "    def forward(self, word, pos_context, neg_contexts):\n",
    "        # word, pos_context and neg_contexts are integers, so can just pick that row from W and C matrices\n",
    "        word_embed = self.W_embeddings(word)\n",
    "        pos_embed = self.C_embeddings(pos_context)\n",
    "        neg_embeds = self.C_embeddings(neg_contexts)\n",
    "        \n",
    "        pos_similarity = torch.mul(word_embed, pos_embed).squeeze()\n",
    "        pos_sum = torch.sum(pos_similarity, dim=1)\n",
    "        pos_logsig = nn.functional.logsigmoid(pos_sum)\n",
    "        pos_score = sum(pos_logsig)\n",
    "        \n",
    "        neg_similarity = torch.bmm(neg_embeds, word_embed.unsqueeze(2)).squeeze()\n",
    "        neg_sum = torch.sum(neg_similarity, dim=1)\n",
    "        neg_logsig = nn.functional.logsigmoid(-1 * neg_sum)\n",
    "        neg_score = sum(neg_logsig)\n",
    "        \n",
    "        loss = -(pos_score+neg_score)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "\n",
    "test_data = get_test_data('lst/lst_test.preprocessed')\n",
    "\n",
    "candidates = get_candidates('lst/lst.gold.candidates')\n",
    "\n",
    "with open('w2i_skipgram.pkl', 'rb') as f:\n",
    "    w2i = pickle.load(f)\n",
    "\n",
    "with open('i2w_skipgram.pkl', 'rb') as f:\n",
    "    i2w = pickle.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda: # load model with gpu (as it was trained)\n",
    "    skipgram_model = torch.load('skipgram.pt')\n",
    "else: # convert to cpu:\n",
    "    skipgram_model = torch.load('skipgram.pt',  map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing - take out any words/candidates aren't present in original corpus (w2i check)  \n",
    "orig_test_data = test_data[:]\n",
    "for [word, sentence_id] in test_data[:]:\n",
    "    word_nopos = word[:-2]\n",
    "    if word_nopos not in w2i:\n",
    "        test_data.remove([word, sentence_id])\n",
    "    for candidate in candidates[word][:]:\n",
    "        if candidate not in w2i:\n",
    "            candidates[word].remove(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert words to indexes\n",
    "\n",
    "word_is = [w2i[word[:-2]] for word, _ in test_data]\n",
    "word_is = torch.LongTensor(word_is)\n",
    "\n",
    "can_is = {}\n",
    "for [word, _] in test_data[:]:\n",
    "    can_i = [w2i[can] for can in candidates[word]]\n",
    "    word_i = w2i[word[:-2]]\n",
    "    can_is[word_i] = torch.LongTensor(can_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranking(word, candidates):\n",
    "\n",
    "    cos = torch.nn.CosineSimilarity(dim=0)\n",
    "    \n",
    "    if use_cuda:\n",
    "        word = word.cuda()\n",
    "\n",
    "    word_embedding = skipgram_model.W_embeddings(word)\n",
    "    can_sims = []\n",
    "    for can in candidates:\n",
    "        if use_cuda:\n",
    "            can = can.cuda()\n",
    "        can_embedding = skipgram_model.W_embeddings(can)\n",
    "        sim = cos(word_embedding, can_embedding)\n",
    "        can_sims.append([can, sim])\n",
    "            \n",
    "    can_sims = sorted(can_sims, key = lambda x: x[1])\n",
    "    return(can_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for i, [word, sentence_id] in enumerate(test_data):\n",
    "    word_i = word_is[i]\n",
    "    cans = can_is[word_i.item()]\n",
    "    rank = get_ranking(word_i, cans)\n",
    "    words_scores = []\n",
    "    for [candidate, score] in rank:\n",
    "        can_word = i2w[candidate.item()]\n",
    "        score = score.item()\n",
    "        words_scores.append([can_word, score])\n",
    "    results[word] = words_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('skipgram_predictions', 'w') as f:\n",
    "    for [word, sentence_id] in orig_test_data: # lst_gap needs all words, also once we do not have data for\n",
    "        f.write('#RANKED\\t')\n",
    "        f.write(word + ' ')\n",
    "        f.write(sentence_id)\n",
    "        if word in results:\n",
    "            for [candidate, score] in results[word]:\n",
    "                f.write('\\t' + candidate + ' ' + str(score))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEAN_GAP\t0.2577320636314508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run lst/lst_gap.py lst/lst_test.gold skipgram_predictions skipgram_out no-mwe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
