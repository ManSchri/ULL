{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# read in data, remove punctuation and make all lower case\n",
    "def read_words(filename):\n",
    "    words = []\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    scount = 0\n",
    "    longest = 0\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        for s in f:\n",
    "            scount += 1\n",
    "            if scount == 100000:\n",
    "                break\n",
    "            clean_s = s.translate(translator).lower()\n",
    "            words.append(clean_s.split())\n",
    "            # keep track of longest sentence\n",
    "            if len(s) > longest:\n",
    "                 longest = len(s)\n",
    "    return words, longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict that returns the index of onehot encoding for a word (and other way around)\n",
    "# also create a frequency dict + set size, usuable for negative sampling\n",
    "import numpy as np\n",
    "\n",
    "def get_onehot_dicts(corpus):\n",
    "    # create one set of all unique words\n",
    "    flat_corpus = [w for s in corpus for w in s]\n",
    "    corpus_set = set(flat_corpus)\n",
    "    w_to_i = {}\n",
    "    i_to_w = {}\n",
    "    w_freq = []\n",
    "    num_words = len(corpus_set)\n",
    "    for i, w in enumerate(corpus_set):\n",
    "        # all indices + 1 to use zero padding later\n",
    "        w_to_i[w] = i + 1\n",
    "        i_to_w[i + 1] = w\n",
    "        freq = flat_corpus.count(w)**0.75\n",
    "        w_freq.append([i, freq])\n",
    "    return w_to_i, i_to_w, np.array(w_freq), num_words+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus created\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-76ecbd3c5097>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mw_to_i1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi_to_w1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_freq1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_words1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_onehot_dicts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml1_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mw_to_i2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi_to_w2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_freq2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_words2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_onehot_dicts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml2_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-871e8e6d4e3b>\u001b[0m in \u001b[0;36mget_onehot_dicts\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mw_to_i\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mi_to_w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mfreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflat_corpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m0.75\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mw_freq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mw_to_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi_to_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_freq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "l1_corpus, longest1 = read_words('hansards/training.en')\n",
    "l2_corpus, longest2 = read_words('hansards/training.fr')\n",
    "# l1_corpus, longest1 = read_words('wa/test.en')\n",
    "# l2_corpus, longest2 = read_words('wa/test.fr')\n",
    "\n",
    "print('corpus created')\n",
    "import time\n",
    "start_time = time.time()\n",
    "w_to_i1, i_to_w1, w_freq1, num_words1 = get_onehot_dicts(l1_corpus)\n",
    "w_to_i2, i_to_w2, w_freq2, num_words2 = get_onehot_dicts(l2_corpus)\n",
    "print(time.time()-start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(l1_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save w_to_i and i_to_w to files\n",
    "import pickle\n",
    "\n",
    "with open('w2i_en_embedalign.pkl', 'wb') as f:\n",
    "    pickle.dump(w_to_i1, f)\n",
    "    \n",
    "with open('i2w_en_embedalign.pkl', 'wb') as f:\n",
    "    pickle.dump(i_to_w1, f)\n",
    "    \n",
    "with open('w2i_fr_embedalign.pkl', 'wb') as f:\n",
    "    pickle.dump(w_to_i2, f)\n",
    "    \n",
    "with open('i2w_fr_embedalign.pkl', 'wb') as f:\n",
    "    pickle.dump(i_to_w2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform corpus to lists of ints\n",
    "import torch\n",
    "\n",
    "# convert to indexes and pad\n",
    "l1_corpus_i = torch.LongTensor([[0] * (longest1-len(sentence)) + [w_to_i1[word] for word in sentence] for sentence in l1_corpus])\n",
    "l2_corpus_i = torch.LongTensor([[0] *(longest2-len(sentence)) + [w_to_i2[word] for word in sentence] for sentence in l2_corpus])\n",
    "\n",
    "del l1_corpus\n",
    "del l2_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "import torch.utils.data\n",
    "from torch.utils.data import sampler\n",
    "from torch.distributions import kl\n",
    "class embed_align(nn.Module):\n",
    "    def __init__(self, vocab_size1, vocab_size2, emb_dimension):\n",
    "        super(embed_align, self).__init__()\n",
    "        self.emb_dimension = emb_dimension\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size1, emb_dimension, padding_idx = 0)\n",
    "        self.BiLSTM = nn.LSTM(emb_dimension, emb_dimension, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.affine1_mu = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_mu = nn.Linear(emb_dimension, emb_dimension)\n",
    "        \n",
    "        self.affine1_sig = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_sig = nn.Linear(emb_dimension, emb_dimension)\n",
    "        \n",
    "        self.affine1_L1 = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_L1 = nn.Linear(emb_dimension, vocab_size1)\n",
    "        self.affine1_L2 = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_L2 = nn.Linear(emb_dimension, vocab_size2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.log_softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, sentence1, sentence2, use_cuda=False):\n",
    "        # sentence1 & sentence2 are (batches of) list of all ints in a sentence\n",
    "        # encoder\n",
    "        sen1_emb = self.embedding(sentence1)\n",
    "        if len(sen1_emb.shape) == 2: # not a batch\n",
    "            sen1_emb = sen1_emb.unsqueeze(0)\n",
    "        h, _ = self.BiLSTM(sen1_emb)\n",
    "        h1, h2 = torch.split(h, split_size_or_sections=self.emb_dimension, dim =2)\n",
    "        h = h1 + h2\n",
    "        mu = self.affine2_mu(self.relu(self.affine1_mu(h)))\n",
    "        sig = self.relu(self.affine2_sig(self.relu(self.affine1_sig(h))))\n",
    "        \n",
    "        sample_norm = dist.multivariate_normal.MultivariateNormal(torch.zeros(self.emb_dimension), torch.eye(self.emb_dimension))\n",
    "        e = sample_norm.sample()\n",
    "        if use_cuda:\n",
    "            z = mu + e.cuda() * sig\n",
    "        else:\n",
    "            z = mu + e * sig\n",
    "    \n",
    "        # likelihood language 1\n",
    "        dist_1 = self.log_softmax(self.affine2_L1(self.relu(self.affine1_L1(z))))\n",
    "        # sum over batch\n",
    "        sum_1 = torch.sum(dist_1, dim=0)\n",
    "        likelihood_1 = torch.mean(sum_1, dim=1)\n",
    "        total_likelihood1 = 0\n",
    "        sen_len = 0\n",
    "        for i, likelihood in enumerate(likelihood_1):\n",
    "            # no batches:\n",
    "            if len(sentence1) == longest1:\n",
    "                if sentence1[i].item() == 0:\n",
    "                    continue\n",
    "                total_likelihood1 += likelihood\n",
    "                sen_len +=1\n",
    "            else:\n",
    "                for j in range(len(sentence1)):\n",
    "                    if sentence1[j][i].item() == 0:\n",
    "                        continue\n",
    "                    total_likelihood1 += likelihood\n",
    "                sen_len +=1\n",
    "        likelihood1 = total_likelihood1/sen_len\n",
    "        \n",
    "        # likelihood language 2\n",
    "        dist_2 = self.log_softmax(self.affine2_L2(self.relu(self.affine1_L2(z))))\n",
    "        sum_2 = torch.sum(dist_2, dim=0)\n",
    "        likelihood_2 = torch.mean(sum_2, dim=1)\n",
    "        total_likelihood2 = 0\n",
    "        sen_len = 0\n",
    "        for i, likelihood in enumerate(likelihood_2):\n",
    "            # no batches:\n",
    "            if len(sentence1) == longest1:\n",
    "                if sentence1[i].item() == 0:\n",
    "                    continue\n",
    "                total_likelihood2 += likelihood\n",
    "                sen_len +=1\n",
    "            else:\n",
    "                for j in range(len(sentence1)):\n",
    "                    if sentence1[j][i].item() == 0:\n",
    "                        continue\n",
    "                    total_likelihood2 += likelihood\n",
    "                sen_len +=1\n",
    "        likelihood2 = total_likelihood2/sen_len\n",
    "        \n",
    "        # KL\n",
    "        # to prevent log returning infinity\n",
    "        sig = sig+1e-8\n",
    "        KL =  -0.5 * torch.sum(1 + torch.log(sig) - mu.pow(2) - sig)\n",
    "        return - ((likelihood1 + likelihood2) - KL)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "ea_model = embed_align(num_words1, num_words2, 200)\n",
    "\n",
    "if use_cuda:\n",
    "    ea_model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(ea_model.parameters(), lr=0.01)\n",
    "ea_model.train()\n",
    "loss_progress = []\n",
    "iter_time = time.time()\n",
    "\n",
    "\n",
    "total_data = torch.utils.data.TensorDataset(l1_corpus_i, l2_corpus_i)\n",
    "dataloader = DataLoader(total_data, batch_size=50)\n",
    "for i, batch in enumerate(dataloader):\n",
    "    batch_l1 = batch[0]\n",
    "    batch_l2 = batch[1]\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if use_cuda:    \n",
    "        loss = ea_model.forward(batch_l1.cuda(), batch_l2.cuda(), use_cuda=True)\n",
    "    else:\n",
    "        loss = ea_model.forward(batch_l1, batch_l2)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 1000 == 0:\n",
    "        loss_progress.append(loss.item())  \n",
    "    if i % 25000 == 0:\n",
    "        print(loss_progress[-1])\n",
    "        print(time.time()-iter_time)\n",
    "        iter_time = time.time()\n",
    "    \n",
    "plt.plot(loss_progress)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ea_model = embed_align(num_words1, num_words2, 200)\n",
    "b = ea_model.forward(l1_corpus_i, l2_corpus_i)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
