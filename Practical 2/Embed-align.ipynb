{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# read in data, remove punctuation and make all lower case\n",
    "def read_words(filename):\n",
    "    words = []\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    scount = 0\n",
    "    with open(filename) as f:\n",
    "        for s in f:\n",
    "            scount += 1\n",
    "            clean_s = s.translate(translator).lower()\n",
    "            words.append(clean_s.split())\n",
    "    return words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict that returns the index of onehot encoding for a word (and other way around)\n",
    "# also create a frequency dict + set size, usuable for negative sampling\n",
    "import numpy as np\n",
    "\n",
    "def get_onehot_dicts(corpus):\n",
    "    # create one set of all unique words\n",
    "    flat_corpus = [w for s in corpus for w in s]\n",
    "    corpus_set = set(flat_corpus)\n",
    "    w_to_i = {}\n",
    "    i_to_w = {}\n",
    "    w_freq = []\n",
    "    num_words = len(corpus_set)\n",
    "    for i, w in enumerate(corpus_set):\n",
    "        w_to_i[w] = i\n",
    "        i_to_w[i] = w\n",
    "        freq = flat_corpus.count(w)**0.75\n",
    "        w_freq.append([i, freq])\n",
    "    return w_to_i, i_to_w, np.array(w_freq), num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus created\n"
     ]
    }
   ],
   "source": [
    "#corpus = read_words('wa/test.en')\n",
    "l1_corpus = read_words('wa/dev.en')\n",
    "l2_corpus = read_words('wa/dev.fr')\n",
    "print('corpus created')\n",
    "w_to_i1, i_to_w1, w_freq1, num_words1 = get_onehot_dicts(l1_corpus)\n",
    "w_to_i2, i_to_w2, w_freq2, num_words2 = get_onehot_dicts(l2_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save w_to_i and i_to_w to files\n",
    "import pickle\n",
    "\n",
    "with open('w2i_en_embedalign.pkl', 'wb') as f:\n",
    "    pickle.dump(w_to_i1, f)\n",
    "    \n",
    "with open('i2w_en_embedalign.pkl', 'wb') as f:\n",
    "    pickle.dump(i_to_w1, f)\n",
    "    \n",
    "with open('w2i_fr_embedalign.pkl', 'wb') as f:\n",
    "    pickle.dump(w_to_i2, f)\n",
    "    \n",
    "with open('i2w_fr_embedalign.pkl', 'wb') as f:\n",
    "    pickle.dump(i_to_w2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "class embed_align(nn.Module):\n",
    "    def __init__(self, vocab_size1, vocab_size2, emb_dimension):\n",
    "        super(embed_align, self).__init__()\n",
    "        self.vocab_size1 = vocab_size1\n",
    "        self.vocab_size2 = vocab_size2\n",
    "        self.emb_dimension = emb_dimension\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size1, emb_dimension)\n",
    "        self.BiLSTM = nn.LSTM(emb_dimension, emb_dimension, bidirectional=True)\n",
    "        \n",
    "        self.affine1_mu = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_mu = nn.Linear(emb_dimension, emb_dimension)\n",
    "        \n",
    "        self.affine1_sig = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_sig = nn.Linear(emb_dimension, emb_dimension)\n",
    "        \n",
    "        self.affine1_L1 = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_L1 = nn.Linear(emb_dimension, vocab_size1)\n",
    "        self.affine1_L2 = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_L2 = nn.Linear(emb_dimension, vocab_size2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "    def forward(self, sentence1, sentence2):\n",
    "        # sentence1 & sentence2 are (batches of) list of all ints in a sentence\n",
    "        # encoder\n",
    "        m = len(sentence1)\n",
    "        sen1_emb = self.embedding(sentence1)\n",
    "        h, _ = self.BiLSTM(sen1_emb.unsqueeze(1))\n",
    "        h1, h2 = torch.split(h, split_size_or_sections=self.emb_dimension, dim =2)\n",
    "        h = h1 + h2\n",
    "        mu = self.affine2_mu(self.relu(self.affine1_mu(h)))\n",
    "        sig = self.relu(self.affine2_sig(self.relu(self.affine1_sig(h))))\n",
    "        \n",
    "        sample_norm = dist.multivariate_normal.MultivariateNormal(torch.zeros(m), torch.eye(m))\n",
    "        e = sample_norm.sample()\n",
    "        \n",
    "        # check if this is correct\n",
    "        z = mu + torch.mv(sig, e)\n",
    "        \n",
    "        # remove once generate part finished\n",
    "        return z\n",
    "    \n",
    "        # generate\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "matrix and vector expected, got 3D, 1D at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524543037166\\work\\aten\\src\\th\\generic/THTensorMath.c:1923",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-7a60f888e7c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mi_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mea_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-45-eb05c979d232>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, sentence1, sentence2)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m# check if this is correct\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmu\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m# remove once generate part finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: matrix and vector expected, got 3D, 1D at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524543037166\\work\\aten\\src\\th\\generic/THTensorMath.c:1923"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ea_model = embed_align(num_words1, num_words2, 200)\n",
    "i_1 = [w_to_i1[w] for w in l1_corpus[0]]\n",
    "i_2 = [w_to_i2[w] for w in l2_corpus[0]]\n",
    "\n",
    "i_1 = torch.LongTensor(i_1)\n",
    "i_2 = torch.LongTensor(i_2)\n",
    "\n",
    "ea_model.forward(i_1, i_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
