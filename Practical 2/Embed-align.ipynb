{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# read in data, remove punctuation and make all lower case\n",
    "def read_words(filename):\n",
    "    words = []\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    scount = 0\n",
    "    longest = 0\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        for s in f:\n",
    "            scount += 1\n",
    "            clean_s = s.translate(translator).lower()\n",
    "            words.append(clean_s.split())\n",
    "            # keep track of longest sentence\n",
    "            if len(s) > longest:\n",
    "                 longest = len(s)\n",
    "    return words, longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict that returns the index of onehot encoding for a word (and other way around)\n",
    "# also create a frequency dict + set size, usuable for negative sampling\n",
    "import numpy as np\n",
    "\n",
    "def get_onehot_dicts(corpus):\n",
    "    # create one set of all unique words\n",
    "    flat_corpus = [w for s in corpus for w in s]\n",
    "    corpus_set = set(flat_corpus)\n",
    "    w_to_i = {}\n",
    "    i_to_w = {}\n",
    "    w_freq = []\n",
    "    num_words = len(corpus_set)\n",
    "    for i, w in enumerate(corpus_set):\n",
    "        # all indices + 1 to use zero padding later\n",
    "        w_to_i[w] = i + 1\n",
    "        i_to_w[i + 1] = w\n",
    "        freq = flat_corpus.count(w)**0.75\n",
    "        w_freq.append([i, freq])\n",
    "    return w_to_i, i_to_w, np.array(w_freq), num_words+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top words\n",
    "# def get_onehot_dicts_sampling(corpus):\n",
    "#     flat_corpus = [w for s in corpus for w in s]\n",
    "#     corpus_set = set(flat_corpus)\n",
    "#     w_to_i = {}\n",
    "#     i_to_w = {}\n",
    "#     w_freq = []\n",
    "\n",
    "#     for i, w in enumerate(corpus_set):\n",
    "#         freq = flat_corpus.count(w)**0.75\n",
    "#         w_freq.append([i, freq])\n",
    "    \n",
    "#     sorted_frequencies = w_freq.sort(key=lambda x: x[1])\n",
    "#     smaller_corpus = [property_a[i] for i in good_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus created\n",
      "0.32686305046081543\n"
     ]
    }
   ],
   "source": [
    "#corpus = read_words('wa/test.en')\n",
    "l1_corpus, longest1 = read_words('wa/test.en')\n",
    "l2_corpus, longest2 = read_words('wa/test.fr')\n",
    "print('corpus created')\n",
    "import time\n",
    "start_time = time.time()\n",
    "w_to_i1, i_to_w1, w_freq1, num_words1 = get_onehot_dicts(l1_corpus)\n",
    "w_to_i2, i_to_w2, w_freq2, num_words2 = get_onehot_dicts(l2_corpus)\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save w_to_i and i_to_w to files\n",
    "import pickle\n",
    "\n",
    "with open('w2i_en_embedalign.pkl', 'wb') as f:\n",
    "    pickle.dump(w_to_i1, f)\n",
    "    \n",
    "with open('i2w_en_embedalign.pkl', 'wb') as f:\n",
    "    pickle.dump(i_to_w1, f)\n",
    "    \n",
    "with open('w2i_fr_embedalign.pkl', 'wb') as f:\n",
    "    pickle.dump(w_to_i2, f)\n",
    "    \n",
    "with open('i2w_fr_embedalign.pkl', 'wb') as f:\n",
    "    pickle.dump(i_to_w2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform corpus to lists of ints\n",
    "import torch\n",
    "\n",
    "# convert to indexes and pad\n",
    "l1_corpus_i = torch.LongTensor([[0] * (longest1-len(sentence)) + [w_to_i1[word] for word in sentence] for sentence in l1_corpus])\n",
    "l2_corpus_i = torch.LongTensor([[0] *(longest2-len(sentence)) + [w_to_i2[word] for word in sentence] for sentence in l2_corpus])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "import torch.utils.data\n",
    "from torch.utils.data import sampler\n",
    "from torch.distributions import kl\n",
    "class embed_align(nn.Module):\n",
    "    def __init__(self, vocab_size1, vocab_size2, emb_dimension):\n",
    "        super(embed_align, self).__init__()\n",
    "        self.vocab_size1 = vocab_size1\n",
    "        self.vocab_size2 = vocab_size2\n",
    "        self.emb_dimension = emb_dimension\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size1, emb_dimension, padding_idx = 0)\n",
    "        self.BiLSTM = nn.LSTM(emb_dimension, emb_dimension, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.affine1_mu = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_mu = nn.Linear(emb_dimension, emb_dimension)\n",
    "        \n",
    "        self.affine1_sig = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_sig = nn.Linear(emb_dimension, emb_dimension)\n",
    "        \n",
    "        self.affine1_L1 = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_L1 = nn.Linear(emb_dimension, vocab_size1)\n",
    "        self.affine1_L2 = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_L2 = nn.Linear(emb_dimension, vocab_size2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.log_softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, sentence1, sentence2, use_cuda=False):\n",
    "        # sentence1 & sentence2 are (batches of) list of all ints in a sentence\n",
    "        # encoder\n",
    "        sen1_emb = self.embedding(sentence1)\n",
    "        if len(sen1_emb.shape) == 2: # not a batch\n",
    "            sen1_emb = sen1_emb.unsqueeze(0)\n",
    "        h, _ = self.BiLSTM(sen1_emb)\n",
    "        h1, h2 = torch.split(h, split_size_or_sections=self.emb_dimension, dim =2)\n",
    "        h = h1 + h2\n",
    "        mu = self.affine2_mu(self.relu(self.affine1_mu(h)))\n",
    "        sig = self.relu(self.affine2_sig(self.relu(self.affine1_sig(h))))\n",
    "        \n",
    "        sample_norm = dist.multivariate_normal.MultivariateNormal(torch.zeros(self.emb_dimension), torch.eye(self.emb_dimension))\n",
    "        e = sample_norm.sample()\n",
    "        if use_cuda:\n",
    "            z = mu + e.cuda() * sig\n",
    "        else:\n",
    "            z = mu + e * sig\n",
    "    \n",
    "        # likelihood language 1\n",
    "        dist_1 = self.log_softmax(self.affine2_L1(self.relu(self.affine1_L1(z))))\n",
    "        # sum over batch\n",
    "        sum_1 = torch.sum(dist_1, dim=0)\n",
    "        likelihood_1 = torch.mean(sum_1, dim=1)\n",
    "        total_likelihood1 = 0\n",
    "        sen_len = 0\n",
    "        for i, likelihood in enumerate(likelihood_1):\n",
    "            # no batches:\n",
    "            if len(sentence1) == longest1:\n",
    "                if sentence1[i].item() == 0:\n",
    "                    continue\n",
    "                total_likelihood1 += likelihood\n",
    "                sen_len +=1\n",
    "            else:\n",
    "                for j in range(len(sentence1)):\n",
    "                    if sentence1[j][i].item() == 0:\n",
    "                        continue\n",
    "                    total_likelihood1 += likelihood\n",
    "                sen_len +=1\n",
    "        likelihood1 = total_likelihood1/sen_len\n",
    "        \n",
    "        # likelihood language 2\n",
    "        dist_2 = self.log_softmax(self.affine2_L2(self.relu(self.affine1_L2(z))))\n",
    "        sum_2 = torch.sum(dist_2, dim=0)\n",
    "        likelihood_2 = torch.mean(sum_2, dim=1)\n",
    "        total_likelihood2 = 0\n",
    "        sen_len = 0\n",
    "        for i, likelihood in enumerate(likelihood_2):\n",
    "            # no batches:\n",
    "            if len(sentence1) == longest1:\n",
    "                if sentence1[i].item() == 0:\n",
    "                    continue\n",
    "                total_likelihood2 += likelihood\n",
    "                sen_len +=1\n",
    "            else:\n",
    "                for j in range(len(sentence1)):\n",
    "                    if sentence1[j][i].item() == 0:\n",
    "                        continue\n",
    "                    total_likelihood2 += likelihood\n",
    "                sen_len +=1\n",
    "        likelihood2 = total_likelihood2/sen_len\n",
    "        \n",
    "        # KL\n",
    "        # to prevent log returning infinity\n",
    "        sig = sig+1e-8\n",
    "        KL =  -0.5 * torch.sum(1 + torch.log(sig) - mu.pow(2) - sig)\n",
    "        return - ((likelihood1 + likelihood2) - KL)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at batch  0\n",
      "0.31984686851501465\n",
      "tensor(1.00000e+06 *\n",
      "       2.6598, device='cuda:0')\n",
      "at batch  10\n",
      "3.5031001567840576\n",
      "tensor(60003.9844, device='cuda:0')\n",
      "at batch  20\n",
      "3.6315455436706543\n",
      "tensor(33596.2930, device='cuda:0')\n",
      "at batch  30\n",
      "3.697462558746338\n",
      "tensor(16170.1621, device='cuda:0')\n",
      "at batch  40\n",
      "3.676900625228882\n",
      "tensor(15591.5479, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "use_cuda = False#torch.cuda.is_available()\n",
    "\n",
    "ea_model = embed_align(num_words1, num_words2, 300)\n",
    "\n",
    "if use_cuda:\n",
    "    ea_model.cuda()\n",
    "optimizer = torch.optim.Adam(ea_model.parameters(), lr=0.01)\n",
    "ea_model.train()\n",
    "loss_progress = []\n",
    "iter_time = time.time()\n",
    "\n",
    "total_data = torch.utils.data.TensorDataset(l1_corpus_i, l2_corpus_i)\n",
    "dataloader = DataLoader(total_data, batch_size=10)\n",
    "for i, batch in enumerate(dataloader):\n",
    "    batch_l1 = batch[0]\n",
    "    batch_l2 = batch[1]\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if use_cuda:    \n",
    "        loss = ea_model.forward(batch_l1.cuda(), batch_l2.cuda(), use_cuda=True)\n",
    "    else:\n",
    "        loss = ea_model.forward(batch_l1, batch_l2)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 10 == 0:\n",
    "        print(\"at batch \", i)\n",
    "        print(time.time()-iter_time)\n",
    "        print(loss)\n",
    "        iter_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.1000e+07)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ea_model = embed_align(num_words1, num_words2, 200)\n",
    "b = ea_model.forward(l1_corpus_i, l2_corpus_i)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
