{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# read in data, remove punctuation and make all lower case\n",
    "def read_words(filename):\n",
    "    words = []\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    scount = 0\n",
    "    longest = 0\n",
    "    w_freq = []\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        for s in f:\n",
    "            scount += 1\n",
    "            if scount == 10000:\n",
    "                break\n",
    "            clean_s = s.translate(translator).lower()\n",
    "            words.append(clean_s.split())\n",
    "\n",
    "            # keep track of longest sentence\n",
    "            if len(s) > longest:\n",
    "                 longest = len(s)\n",
    "    return words, longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict that returns the index of onehot encoding for a word (and other way around)\n",
    "# also create a frequency dict + set size, usuable for negative sampling\n",
    "# import numpy as np\n",
    "\n",
    "# def get_onehot_dicts(corpus):\n",
    "#     # create one set of all unique words\n",
    "#     flat_corpus = [w for s in corpus for w in s]\n",
    "#     corpus_set = set(flat_corpus)\n",
    "#     w_to_i = {}\n",
    "#     i_to_w = {}\n",
    "#     w_freq = []\n",
    "#     num_words = len(corpus_set)\n",
    "#     for i, w in enumerate(corpus_set):\n",
    "#         # all indices + 1 to use zero padding later\n",
    "#         w_to_i[w] = i + 1\n",
    "#         i_to_w[i + 1] = w\n",
    "#         freq = flat_corpus.count(w)**0.75\n",
    "#         w_freq.append([i, freq])\n",
    "#     return w_to_i, i_to_w, np.array(w_freq), num_words+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "def get_onehot_dicts(corpus):\n",
    "    # create one set of all unique words\n",
    "    flat_corpus = [w for s in corpus for w in s]\n",
    "    corpus_set = set(flat_corpus)\n",
    "    w_to_i = defaultdict(int)\n",
    "    i_to_w = defaultdict(lambda: '<unk>')\n",
    "    w_freq = {}\n",
    "    for i, w in enumerate(corpus_set):\n",
    "        freq = flat_corpus.count(w)**0.75\n",
    "        w_freq[w] = freq\n",
    "    sorted_corp = sorted(corpus_set, key=lambda x: w_freq[x], reverse=True)\n",
    "\n",
    "    for i, w in enumerate(sorted_corp[:500]):\n",
    "        w_to_i[w] = i+1\n",
    "        i_to_w[i+1] = w\n",
    "    num_words = len(w_to_i)\n",
    "\n",
    "    return w_to_i, i_to_w, np.array(w_freq), num_words+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus created\n",
      "lang.1 done\n",
      "61.90531253814697\n"
     ]
    }
   ],
   "source": [
    "l1_corpus, longest1 = read_words('hansards/training.en')\n",
    "l2_corpus, longest2 = read_words('hansards/training.fr')\n",
    "# l1_corpus, longest1 = read_words('wa/test.en')\n",
    "# l2_corpus, longest2 = read_words('wa/test.fr')\n",
    "\n",
    "print('corpus created')\n",
    "import time\n",
    "start_time = time.time()\n",
    "w_to_i1, i_to_w1, w_freq1, num_words1 = get_onehot_dicts(l1_corpus)\n",
    "print('lang.1 done')\n",
    "w_to_i2, i_to_w2, w_freq2, num_words2 = get_onehot_dicts(l2_corpus)\n",
    "print(time.time()-start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n",
      "501\n",
      "501\n"
     ]
    }
   ],
   "source": [
    "print(len(l1_corpus))\n",
    "print(num_words1)\n",
    "print(num_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save w_to_i and i_to_w to files\n",
    "import dill\n",
    "\n",
    "with open('w2i_en_embedalign.pkl', 'wb') as f:\n",
    "    dill.dump(w_to_i1, f)\n",
    "    \n",
    "with open('i2w_en_embedalign.pkl', 'wb') as f:\n",
    "    dill.dump(i_to_w1, f)\n",
    "    \n",
    "with open('w2i_fr_embedalign.pkl', 'wb') as f:\n",
    "    dill.dump(w_to_i2, f)\n",
    "    \n",
    "with open('i2w_fr_embedalign.pkl', 'wb') as f:\n",
    "    dill.dump(i_to_w2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform corpus to lists of ints\n",
    "import torch\n",
    "\n",
    "# convert to indexes and pad\n",
    "l1_corpus_i = torch.LongTensor([[0] * (longest1-len(sentence)) + [w_to_i1[word] for word in sentence] for sentence in l1_corpus])\n",
    "l2_corpus_i = torch.LongTensor([[0] *(longest2-len(sentence)) + [w_to_i2[word] for word in sentence] for sentence in l2_corpus])\n",
    "\n",
    "del l1_corpus\n",
    "del l2_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "import torch.utils.data\n",
    "from torch.utils.data import sampler\n",
    "from torch.distributions import kl\n",
    "class embed_align(nn.Module):\n",
    "    def __init__(self, vocab_size1, vocab_size2, emb_dimension):\n",
    "        super(embed_align, self).__init__()\n",
    "        self.emb_dimension = emb_dimension\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size1, emb_dimension, padding_idx = 0)\n",
    "        self.BiLSTM = nn.LSTM(emb_dimension, emb_dimension, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.affine1_mu = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_mu = nn.Linear(emb_dimension, emb_dimension)\n",
    "        \n",
    "        self.affine1_sig = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_sig = nn.Linear(emb_dimension, emb_dimension)\n",
    "        \n",
    "        self.affine1_L1 = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_L1 = nn.Linear(emb_dimension, vocab_size1)\n",
    "        self.affine1_L2 = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_L2 = nn.Linear(emb_dimension, vocab_size2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.log_softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, sentence1, sentence2, use_cuda=False):\n",
    "        # sentence1 & sentence2 are (batches of) list of all ints in a sentence\n",
    "        # encoder\n",
    "        sen1_emb = self.embedding(sentence1)\n",
    "        if len(sen1_emb.shape) == 2: # not a batch\n",
    "            sen1_emb = sen1_emb.unsqueeze(0)\n",
    "        h, _ = self.BiLSTM(sen1_emb)\n",
    "        h1, h2 = torch.split(h, split_size_or_sections=self.emb_dimension, dim =2)\n",
    "        h = h1 + h2\n",
    "        mu = self.affine2_mu(self.relu(self.affine1_mu(h)))\n",
    "        sig = self.relu(self.affine2_sig(self.relu(self.affine1_sig(h))))\n",
    "        \n",
    "        sample_norm = dist.multivariate_normal.MultivariateNormal(torch.zeros(self.emb_dimension), torch.eye(self.emb_dimension))\n",
    "        e = sample_norm.sample()\n",
    "        if use_cuda:\n",
    "            z = mu + e.cuda() * sig\n",
    "        else:\n",
    "            z = mu + e * sig\n",
    "    \n",
    "        # likelihood language 1\n",
    "        dist_1 = self.log_softmax(self.affine2_L1(self.relu(self.affine1_L1(z))))\n",
    "        # sum over batch\n",
    "        sum_1 = torch.sum(dist_1, dim=0)\n",
    "        likelihood_1 = torch.mean(sum_1, dim=1)\n",
    "        total_likelihood1 = 0\n",
    "        sen_len = 0\n",
    "        for i, likelihood in enumerate(likelihood_1):\n",
    "            # no batches:\n",
    "            if len(sentence1) == longest1:\n",
    "                if sentence1[i].item() == 0:\n",
    "                    continue\n",
    "                total_likelihood1 += likelihood\n",
    "                sen_len +=1\n",
    "            else:\n",
    "                for j in range(len(sentence1)):\n",
    "                    if sentence1[j][i].item() == 0:\n",
    "                        continue\n",
    "                    total_likelihood1 += likelihood\n",
    "                sen_len +=1\n",
    "        likelihood1 = total_likelihood1/sen_len\n",
    "        \n",
    "        # likelihood language 2\n",
    "        dist_2 = self.log_softmax(self.affine2_L2(self.relu(self.affine1_L2(z))))\n",
    "        sum_2 = torch.sum(dist_2, dim=0)\n",
    "        likelihood_2 = torch.mean(sum_2, dim=1)\n",
    "        total_likelihood2 = 0\n",
    "        sen_len = 0\n",
    "        for i, likelihood in enumerate(likelihood_2):\n",
    "            # no batches:\n",
    "            if len(sentence1) == longest1:\n",
    "                if sentence1[i].item() == 0:\n",
    "                    continue\n",
    "                total_likelihood2 += likelihood\n",
    "                sen_len +=1\n",
    "            else:\n",
    "                for j in range(len(sentence1)):\n",
    "                    if sentence1[j][i].item() == 0:\n",
    "                        continue\n",
    "                    total_likelihood2 += likelihood\n",
    "                sen_len +=1\n",
    "        likelihood2 = total_likelihood2/sen_len\n",
    "        \n",
    "        # KL\n",
    "        # to prevent log returning infinity\n",
    "        sig = sig+1e-8\n",
    "        KL =  -0.5 * torch.sum(1 + torch.log(sig) - mu.pow(2) - sig)\n",
    "        return - ((likelihood1 + likelihood2) - KL)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524543037166\\work\\aten\\src\\thc\\generic/THCTensorMath.cu:35",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-a43461aba477>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mea_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mea_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    247\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNNBase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mflatten_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m                     \u001b[0mweight_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_stride0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cudnn_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                     self.batch_first, bool(self.bidirectional))\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_param_buf_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight_buf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524543037166\\work\\aten\\src\\thc\\generic/THCTensorMath.cu:35"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "ea_model = embed_align(num_words1, num_words2, 200)\n",
    "\n",
    "if use_cuda:\n",
    "    ea_model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(ea_model.parameters(), lr=0.01)\n",
    "ea_model.train()\n",
    "loss_progress = []\n",
    "iter_time = time.time()\n",
    "\n",
    "\n",
    "total_data = torch.utils.data.TensorDataset(l1_corpus_i, l2_corpus_i)\n",
    "dataloader = DataLoader(total_data, batch_size=50)\n",
    "for i, batch in enumerate(dataloader):\n",
    "    batch_l1 = batch[0]\n",
    "    batch_l2 = batch[1]\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if use_cuda:    \n",
    "        loss = ea_model.forward(batch_l1.cuda(), batch_l2.cuda(), use_cuda=True)\n",
    "    else:\n",
    "        loss = ea_model.forward(batch_l1, batch_l2)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 1000 == 0:\n",
    "        loss_progress.append(loss.item())  \n",
    "    if i % 25000 == 0:\n",
    "        print(loss_progress[-1])\n",
    "        print(time.time()-iter_time)\n",
    "        iter_time = time.time()\n",
    "    \n",
    "plt.plot(loss_progress)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ea_model = embed_align(num_words1, num_words2, 200)\n",
    "b = ea_model.forward(l1_corpus_i, l2_corpus_i)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
