{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "import torch.utils.data\n",
    "from torch.utils.data import sampler\n",
    "from torch.distributions import kl\n",
    "class embed_align(nn.Module):\n",
    "    def __init__(self, vocab_size1, vocab_size2, emb_dimension):\n",
    "        super(embed_align, self).__init__()\n",
    "        self.emb_dimension = emb_dimension\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size1, emb_dimension, padding_idx = 0)\n",
    "        self.BiLSTM = nn.LSTM(emb_dimension, emb_dimension, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.affine1_mu = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_mu = nn.Linear(emb_dimension, emb_dimension)\n",
    "        \n",
    "        self.affine1_sig = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_sig = nn.Linear(emb_dimension, emb_dimension)\n",
    "        \n",
    "        self.affine1_L1 = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_L1 = nn.Linear(emb_dimension, vocab_size1)\n",
    "        self.affine1_L2 = nn.Linear(emb_dimension, emb_dimension)\n",
    "        self.affine2_L2 = nn.Linear(emb_dimension, vocab_size2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.log_softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, sentence1, sentence2, use_cuda=False):\n",
    "        # sentence1 & sentence2 are (batches of) list of all ints in a sentence\n",
    "        # encoder\n",
    "        sen1_emb = self.embedding(sentence1)\n",
    "        if len(sen1_emb.shape) == 2: # not a batch\n",
    "            sen1_emb = sen1_emb.unsqueeze(0)\n",
    "        h, _ = self.BiLSTM(sen1_emb)\n",
    "        h1, h2 = torch.split(h, split_size_or_sections=self.emb_dimension, dim =2)\n",
    "        h = h1 + h2\n",
    "        mu = self.affine2_mu(self.relu(self.affine1_mu(h)))\n",
    "        sig = self.relu(self.affine2_sig(self.relu(self.affine1_sig(h))))\n",
    "        \n",
    "        sample_norm = dist.multivariate_normal.MultivariateNormal(torch.zeros(self.emb_dimension), torch.eye(self.emb_dimension))\n",
    "        e = sample_norm.sample()\n",
    "        if use_cuda:\n",
    "            z = mu + e.cuda() * sig\n",
    "        else:\n",
    "            z = mu + e * sig\n",
    "    \n",
    "        # likelihood language 1\n",
    "        dist_1 = self.log_softmax(self.affine2_L1(self.relu(self.affine1_L1(z))))\n",
    "        # sum over batch\n",
    "        sum_1 = torch.sum(dist_1, dim=0)\n",
    "        likelihood_1 = torch.mean(sum_1, dim=1)\n",
    "        total_likelihood1 = 0\n",
    "        sen_len = 0\n",
    "        for i, likelihood in enumerate(likelihood_1):\n",
    "            # no batches:\n",
    "            if len(sentence1) == longest1:\n",
    "                if sentence1[i].item() == 0:\n",
    "                    continue\n",
    "                total_likelihood1 += likelihood\n",
    "                sen_len +=1\n",
    "            else:\n",
    "                for j in range(len(sentence1)):\n",
    "                    if sentence1[j][i].item() == 0:\n",
    "                        continue\n",
    "                    total_likelihood1 += likelihood\n",
    "                sen_len +=1\n",
    "        likelihood1 = total_likelihood1/sen_len\n",
    "        \n",
    "        # likelihood language 2\n",
    "        dist_2 = self.log_softmax(self.affine2_L2(self.relu(self.affine1_L2(z))))\n",
    "        sum_2 = torch.sum(dist_2, dim=0)\n",
    "        likelihood_2 = torch.mean(sum_2, dim=1)\n",
    "        total_likelihood2 = 0\n",
    "        sen_len = 0\n",
    "        for i, likelihood in enumerate(likelihood_2):\n",
    "            # no batches:\n",
    "            if len(sentence1) == longest1:\n",
    "                if sentence1[i].item() == 0:\n",
    "                    continue\n",
    "                total_likelihood2 += likelihood\n",
    "                sen_len +=1\n",
    "            else:\n",
    "                for j in range(len(sentence1)):\n",
    "                    if sentence1[j][i].item() == 0:\n",
    "                        continue\n",
    "                    total_likelihood2 += likelihood\n",
    "                sen_len +=1\n",
    "        likelihood2 = total_likelihood2/sen_len\n",
    "        \n",
    "        # KL\n",
    "        # to prevent log returning infinity\n",
    "        sig = sig+1e-8\n",
    "        KL =  -0.5 * torch.sum(1 + torch.log(sig) - mu.pow(2) - sig)\n",
    "        return - ((likelihood1 + likelihood2) - KL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all needed data\n",
    "\n",
    "def get_test_data(filename):\n",
    "    test_data = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line = line.split('\\t')\n",
    "            test_word = line[0]\n",
    "            sentence_id = line[1]\n",
    "            test_data.append([test_word, sentence_id])\n",
    "    return(test_data)\n",
    "\n",
    "def get_candidates(filename):\n",
    "    with open(filename) as f:\n",
    "        candidate_dict = {}\n",
    "        for line in f:\n",
    "            line = line.split('::')\n",
    "            word = line[0]\n",
    "            candidates = line[1].split(';')\n",
    "            candidate_dict[word] = candidates\n",
    "        return candidate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import torch\n",
    "\n",
    "test_data = get_test_data('lst/lst_test.preprocessed')\n",
    "\n",
    "candidates = get_candidates('lst/lst.gold.candidates')\n",
    "\n",
    "with open('w2i_en_embedalign.pkl', 'rb') as f:\n",
    "    w2i_en = dill.load(f)\n",
    "\n",
    "with open('i2w_en_embedalign.pkl', 'rb') as f:\n",
    "    i2w_en = dill.load(f)\n",
    "    \n",
    "with open('w2i_fr_embedalign.pkl', 'rb') as f:\n",
    "    w2i_fr = dill.load(f)\n",
    "\n",
    "with open('i2w_fr_embedalign.pkl', 'rb') as f:\n",
    "    i2w_fr = dill.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "id_to_sentence = {}\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "with open('lst/lst_test.preprocessed') as f:\n",
    "    for line in f:\n",
    "        line = line.split('\\t')\n",
    "        test_word = line[0]\n",
    "        sentence_id = line[1]\n",
    "        sentence = line[3].translate(translator).lower().split()\n",
    "        #print(sentence)\n",
    "        id_to_sentence[int(sentence_id)] = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['although',\n",
       " 'the',\n",
       " 'mah',\n",
       " 'look',\n",
       " 'family',\n",
       " 'became',\n",
       " 'a',\n",
       " 'strong',\n",
       " 'part',\n",
       " 'of',\n",
       " 'community',\n",
       " 'life',\n",
       " 'in',\n",
       " 'the',\n",
       " 'king',\n",
       " 'valley',\n",
       " 'they',\n",
       " 'had',\n",
       " 'to',\n",
       " 'suffer',\n",
       " 'the',\n",
       " 'occasional',\n",
       " 'racist',\n",
       " 'comments',\n",
       " 'which',\n",
       " 'must',\n",
       " 'have',\n",
       " 'made',\n",
       " 'life',\n",
       " 'unpleasant',\n",
       " 'at',\n",
       " 'times']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_sentence[1105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda: # load model with gpu (as it was trained)\n",
    "    embedalign_model = torch.load('embedalign.pt')\n",
    "else: # convert to cpu:\n",
    "    embedalign_model = torch.load('embedalign.pt',  map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1710\n",
      "4207\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))\n",
    "\n",
    "tot_cans = 0\n",
    "\n",
    "for cans in candidates.values():\n",
    "    tot_cans += len(cans)\n",
    "    \n",
    "print(tot_cans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_test_data = test_data[:]\n",
    "for [word, sentence_id] in test_data[:]:\n",
    "    word_nopos = word[:-2]\n",
    "    if word_nopos not in w2i_en:\n",
    "        test_data.remove([word, sentence_id])\n",
    "    for candidate in candidates[word][:]:\n",
    "        if candidate not in w2i_en:\n",
    "            candidates[word].remove(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540\n",
      "1142\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))\n",
    "\n",
    "tot_cans = 0\n",
    "\n",
    "for cans in candidates.values():\n",
    "    tot_cans += len(cans)\n",
    "    \n",
    "print(tot_cans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_is = [w2i_en[word[:-2]] for word, _ in test_data]\n",
    "if use_cuda:\n",
    "    word_is = torch.cuda.LongTensor(word_is)\n",
    "else:\n",
    "    word_is = torch.LongTensor(word_is)\n",
    "\n",
    "\n",
    "can_is = {}\n",
    "for [word, _] in test_data[:]:\n",
    "    can_i = [w2i_en[can] for can in candidates[word]]\n",
    "    word_i = w2i_en[word[:-2]]\n",
    "    if use_cuda:\n",
    "        can_is[word_i] = torch.cuda.LongTensor(can_i)\n",
    "    else:\n",
    "        can_is[word_i] = torch.LongTensor(can_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_is = {}\n",
    "for [word, sentence_id] in test_data[:]:\n",
    "    \n",
    "    sen_i = [w2i_en[sen] for sen in id_to_sentence[int(sentence_id)]]\n",
    "    #print(sentence_id,id_to_sentence[int(sentence_id)], sen_i)\n",
    "    if use_cuda:\n",
    "        sentence_is[int(sentence_id)] = torch.cuda.LongTensor(sen_i)\n",
    "    else:\n",
    "        sentence_is[int(sentence_id)] = torch.LongTensor(sen_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranking(word, candidates, sentence, word_pos):\n",
    "    mu, sigma = embedalign_step(embedalign_model, sentence)\n",
    "\n",
    "    can_sims=[]\n",
    "    for can in candidates:\n",
    "        candidate_sentence = sentence\n",
    "        candidate_sentence[word_pos] = can\n",
    "        \n",
    "        mu_p, sigma_p = embedalign_step(embedalign_model, candidate_sentence)\n",
    "        sigma_p = sigma_p + 1e-8\n",
    "        sigma = sigma + 1e-8\n",
    "        score =  (torch.log(sigma_p) - torch.log(sigma) + (sigma**2 + (mu - mu_p)**2) / (2*sigma_p**2) - 0.5)\n",
    "        score = score.sum().data.item()\n",
    "        can_sims.append([can, score])\n",
    "            \n",
    "    return(can_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedalign_step(model, sentence):\n",
    "\n",
    "    # Get embeddings for words in sentence\n",
    "    sen_emb = model.embedding(sentence)\n",
    "    # - Encoder -\n",
    "    if len(sen_emb.shape) == 2: # not a batch\n",
    "        sen_emb = sen_emb.unsqueeze(0)\n",
    "    h, _ = model.BiLSTM(sen_emb)\n",
    "        \n",
    "    h1, h2 = torch.split(h, split_size_or_sections=model.emb_dimension, dim =2)\n",
    "    h = h1 + h2\n",
    "    \n",
    "    \n",
    "    mu = model.affine2_mu(model.relu(model.affine1_mu(h)))\n",
    "    sigma = model.relu(model.affine2_sig(model.relu(model.affine1_sig(h))))\n",
    "\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for i, [word, sentence_id] in enumerate(test_data):\n",
    "    word_i = word_is[i]\n",
    "    cans = can_is[word_i.item()]\n",
    "    \n",
    "    sentence_ids = sentence_is[int(sentence_id)]\n",
    "    \n",
    "    word_pos = -1\n",
    "    for e, el in enumerate(sentence_ids):\n",
    "        if el == word_i:\n",
    "            word_pos = e\n",
    "        \n",
    "    \n",
    "    if (len(cans)>0):\n",
    "        rank = get_ranking(word_i, cans, sentence_ids, word_pos)\n",
    "        words_scores = []\n",
    "        for [candidate, score] in rank:\n",
    "            can_word = i2w_en[int(candidate)]\n",
    "            score = score\n",
    "            words_scores.append([can_word, score])\n",
    "        results[word] = words_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embedalign_predictions', 'w') as f:\n",
    "    for [word, sentence_id] in orig_test_data: # lst_gap needs all words, also once we do not have data for\n",
    "        f.write('#RANKED\\t')\n",
    "        f.write(word + ' ')\n",
    "        f.write(sentence_id)\n",
    "        if word in results:\n",
    "            for [candidate, score] in results[word]:\n",
    "                f.write('\\t' + candidate + ' ' + str(score))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEAN_GAP\t0.033760420625893135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run lst/lst_gap.py lst/lst_test.gold embedalign_predictions embed_align_out no-mwe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEAN_GAP\t0.06320463650142717\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run lst/lst_gap.py lst/lst_test.gold bayesian_skipgram_predictions bayesian_skipgram_out no-mwe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alignments(model, sentence1, sentence2):\n",
    "    mu, sigma = embedalign_step(model, sentence1)\n",
    "\n",
    "    softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "    sample_norm = dist.multivariate_normal.MultivariateNormal(torch.zeros(model.emb_dimension), torch.eye(model.emb_dimension))\n",
    "    e = sample_norm.sample()\n",
    "    if use_cuda:\n",
    "        z = mu + e.cuda() * sigma\n",
    "    else:\n",
    "        z = mu + e * sigma\n",
    "    g = model.log_softmax(model.affine2_L2(model.relu(model.affine1_L2(z))))\n",
    "    g = g.squeeze(0)\n",
    "    x = model.affine2_L2(model.relu(model.affine1_L2(z)))\n",
    "\n",
    "    translations = []\n",
    "    for i, word1 in enumerate(sentence1):\n",
    "        highest_prob = -100000\n",
    "        highest_prob_word = -1\n",
    "        for j, word2 in enumerate(sentence2):\n",
    "            prob = g[i][word2]\n",
    "            if prob> highest_prob:\n",
    "                highest_prob = prob\n",
    "                highest_prob_word = j + 1 # because test.naacl starts counting at 1\n",
    "        translations.append([i+ 1, highest_prob_word])\n",
    "        \n",
    "    return(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        for s in f:\n",
    "            data.append(s.split())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en = read_data('wa/test.en')\n",
    "data_fr = read_data('wa/test.fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "translations = []\n",
    "for i, sen_en in enumerate(data_en):\n",
    "    if use_cuda:\n",
    "        sen_en_i = torch.cuda.LongTensor([w2i_en[word] for word in sen_en])\n",
    "        sen_fr_i = torch.cuda.LongTensor([w2i_fr[word] for word in data_fr[i]])\n",
    "    else:\n",
    "        sen_en_i = torch.LongTensor([w2i_en[word] for word in sen_en])\n",
    "        sen_fr_i = torch.LongTensor([w2i_fr[word] for word in data_fr[i]])\n",
    "\n",
    "    translation = get_alignments(embedalign_model, sen_en_i, sen_fr_i)\n",
    "    translations.append(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('aer_predictions.naacl', 'w') as f:\n",
    "    for i in range(len(data_en)):\n",
    "        for pair in translations[i]:\n",
    "            f.write(str(i) + ' ' + str(pair[0]) + ' ' + str(pair[1]) + ' S \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9206803499877341\n"
     ]
    }
   ],
   "source": [
    "%run wa/aer.py aer_predictions.naacl wa/test.naacl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
