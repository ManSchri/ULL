{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import distributions\n",
    "\n",
    "class bayesian_skipgram(nn.Module):\n",
    "    def __init__(self, num_words, emb_dim):\n",
    "        super(bayesian_skipgram, self).__init__()\n",
    "\n",
    "        self.num_words = num_words\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.R = nn.Embedding(num_words, emb_dim)\n",
    "        self.mu_prior = nn.Embedding(num_words, emb_dim)\n",
    "        self.sigma_prior  = nn.Embedding(num_words, emb_dim)\n",
    "        \n",
    "        self.M = nn.Linear(2*emb_dim, 2*emb_dim)\n",
    "        self.affine_lambda_mu = nn.Linear(2*emb_dim, emb_dim)\n",
    "        self.affine_lambda_sigma = nn.Linear(2*emb_dim, emb_dim)\n",
    "        self.affine_theta = nn.Linear(emb_dim, num_words)\n",
    "        \n",
    "\n",
    "    def forward(self, word_idx, context_idx):\n",
    "        \n",
    "        batch_size = word_idx.shape[0]\n",
    "        n_context = len(context_idx[0])\n",
    "        \n",
    "        # ********** Encoder ************\n",
    "        \n",
    "        R_w = self.R(word_idx) \n",
    "        R_w = R_w.view(batch_size, 1, self.emb_dim) \n",
    "        R_w = R_w.repeat(1, n_context, 1) \n",
    "        \n",
    "        R_cj = self.R(context_idx)\n",
    "        \n",
    "        \n",
    "        RcRw = torch.cat((R_w, R_cj), dim=2)\n",
    "    \n",
    "        h = nn.ReLU()(self.M(RcRw)) \n",
    "        h = torch.sum(h, dim=1)   \n",
    "\n",
    "        mu = self.affine_lambda_mu(h)\n",
    "        sigma = nn.functional.softplus(self.affine_lambda_sigma(h))\n",
    "\n",
    "        # reparametrization trick\n",
    "        eps = distributions.MultivariateNormal(torch.zeros(self.emb_dim), torch.eye(self.emb_dim)).sample()\n",
    "        z = mu + sigma * eps    \n",
    "               \n",
    "            \n",
    "        # ********* Decoder ***********\n",
    "    \n",
    "        affine_categ = self.affine_theta(z)\n",
    "        f_i = nn.functional.softmax(affine_categ, dim=1)    \n",
    "                            \n",
    "        mu_prior = self.mu_prior(word_idx)\n",
    "        sigma_prior = nn.functional.softplus(self.sigma_prior(word_idx))\n",
    "            \n",
    "        # ********** Loss ************\n",
    "        \n",
    "        likelihood_terms = torch.zeros(batch_size)\n",
    "        KL_div_terms = torch.zeros(batch_size)\n",
    "        \n",
    "        for i, contexts in enumerate(context_idx):  \n",
    "            likelihood = 0\n",
    "            for idx in contexts:\n",
    "                likelihood += torch.log(f_i[i, idx] +1e-8)\n",
    "            likelihood_terms[i] = likelihood\n",
    "            \n",
    "            KL =  self.KL_div(mu_prior[i], sigma_prior[i],  mu[i],  sigma[i] )\n",
    "            KL_div_terms[i] = KL\n",
    "            \n",
    "          \n",
    "        total_loss = torch.mean(KL_div_terms) - torch.mean(likelihood_terms)\n",
    "             \n",
    "        return total_loss\n",
    "    \n",
    "    def KL_div(self,  mu_p, sigma_p, mu, sigma):\n",
    "        div = torch.log(sigma_p + 1e-8) - torch.log(sigma+1e-8) + (sigma**2 + (mu - mu_p)**2) / (2*sigma_p**2) - 0.5\n",
    "        return div.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all needed data\n",
    "\n",
    "def get_test_data(filename):\n",
    "    test_data = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line = line.split('\\t')\n",
    "            test_word = line[0]\n",
    "            sentence_id = line[1]\n",
    "            test_data.append([test_word, sentence_id])\n",
    "    return(test_data)\n",
    "\n",
    "def get_candidates(filename):\n",
    "    with open(filename) as f:\n",
    "        candidate_dict = {}\n",
    "        for line in f:\n",
    "            line = line.split('::')\n",
    "            word = line[0]\n",
    "            candidates = line[1].split(';')\n",
    "            candidate_dict[word] = candidates\n",
    "        return candidate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "\n",
    "test_data = get_test_data('lst/lst_test.preprocessed')\n",
    "\n",
    "candidates = get_candidates('lst/lst.gold.candidates')\n",
    "\n",
    "with open('w2i_bayesian.pkl', 'rb') as f:\n",
    "    w2i = pickle.load(f)\n",
    "\n",
    "with open('i2w_bayesian.pkl', 'rb') as f:\n",
    "    i2w = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sindi\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:333: UserWarning: Couldn't retrieve source code for container of type bayesian_skipgram. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda: # load model with gpu (as it was trained)\n",
    "    bayesian_skipgram_model = torch.load('bayesian_skipgram_20000_lr5-3.pt')\n",
    "else: # convert to cpu:\n",
    "    bayesian_skipgram_model = torch.load('bayesian_skipgram_20000_lr5-3.pt',  map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing - take out any words/candidates aren't present in original corpus (w2i check)  \n",
    "orig_test_data = test_data[:]\n",
    "for [word, sentence_id] in test_data[:]:\n",
    "    word_nopos = word[:-2]\n",
    "    if word_nopos not in w2i:\n",
    "        test_data.remove([word, sentence_id])\n",
    "    for candidate in candidates[word][:]:\n",
    "        if candidate not in w2i:\n",
    "            candidates[word].remove(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert words to indexes\n",
    "\n",
    "word_is = [w2i[word[:-2]] for word, _ in test_data]\n",
    "word_is = torch.LongTensor(word_is)\n",
    "\n",
    "\n",
    "\n",
    "can_is = {}\n",
    "for [word, _] in test_data[:]:\n",
    "    can_i = [w2i[can] for can in candidates[word]]\n",
    "    word_i = w2i[word[:-2]]\n",
    "    can_is[word_i] = torch.LongTensor(can_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bsg_step(model, word, context):\n",
    "    word_emb = model.R(word)\n",
    "    context_embs = model.R(context)\n",
    "    emb_dim = model.emb_dim\n",
    "    n_context = context_embs.shape[0]\n",
    "        \n",
    "    \n",
    "    word_emb = word_emb.view(1, emb_dim) \n",
    "    word_emb = word_emb.repeat(n_context, 1) \n",
    "                \n",
    "    cat_embs = torch.cat((word_emb, context_embs), dim=-1)\n",
    "    \n",
    "    h = nn.ReLU()(model.M(cat_embs)) \n",
    "    h = torch.sum(h, dim=0)   \n",
    "    \n",
    "    mu = model.affine_lambda_mu(h)\n",
    "    sigma = nn.functional.softplus(model.affine_lambda_sigma(h))\n",
    "\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranking(word, candidates):\n",
    "    \n",
    "    mu, sigma = bsg_step(bayesian_skipgram_model, word, candidates)\n",
    "    #print('mu, sig', mu.shape, sigma.shape, mu, sigma)\n",
    "    can_sims=[]\n",
    "    for can in candidates:\n",
    "        \n",
    "        mu_p = bayesian_skipgram_model.mu_prior(can)\n",
    "        sigma_p = nn.functional.softplus(bayesian_skipgram_model.sigma_prior(can))\n",
    "        score = bayesian_skipgram_model.KL_div(mu_p, sigma_p, mu, sigma)#torch.log(sigma_x) - torch.log(sigma) + 0.5 * (sigma**2 + (mu - mu_x)**2)/sigma_x**2 - 0.5\n",
    "        score = score.sum().data.item()\n",
    "        can_sims.append([can, score])\n",
    "            \n",
    "    #can_sims = sorted(can_sims, key = lambda x: x[1])\n",
    "    return(can_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for i, [word, sentence_id] in enumerate(test_data):\n",
    "    word_i = word_is[i]\n",
    "    cans = can_is[word_i.item()]\n",
    "    if (len(cans)>0):\n",
    "        rank = get_ranking(word_i, cans)\n",
    "        words_scores = []\n",
    "        for [candidate, score] in rank:\n",
    "            can_word = i2w[int(candidate)]\n",
    "            score = score\n",
    "            words_scores.append([can_word, score])\n",
    "        results[word] = words_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bayesian_skipgram_predictions', 'w') as f:\n",
    "    for [word, sentence_id] in orig_test_data: # lst_gap needs all words, also once we do not have data for\n",
    "        f.write('#RANKED\\t')\n",
    "        f.write(word + ' ')\n",
    "        f.write(sentence_id)\n",
    "        if word in results:\n",
    "            for [candidate, score] in results[word]:\n",
    "                f.write('\\t' + candidate + ' ' + str(score))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEAN_GAP\t0.25065767465893196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run lst/lst_gap.py lst/lst_test.gold bayesian_skipgram_predictions bayesian_skipgram_out no-mwe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
