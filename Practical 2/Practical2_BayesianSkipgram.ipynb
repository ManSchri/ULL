{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# read in data, remove punctuation and make all lower case\n",
    "def read_corpus(filename):\n",
    "    corpus = []\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    with open(filename) as f:\n",
    "        for s in f:\n",
    "            clean_s = s.translate(translator).lower()\n",
    "            clean_words = clean_s.split()\n",
    "            corpus.append(clean_words)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "#Make data dictionaries and indexing \n",
    "def make_dictionaries(corpus):\n",
    "    # create one set of all unique words\n",
    "    flat_corpus = [w for s in corpus for w in s]\n",
    "    corpus_set = set(flat_corpus)\n",
    "    w_to_i = {}\n",
    "    i_to_w = {}\n",
    "    w_freq = []\n",
    "    num_words = len(corpus_set)\n",
    "    for i, w in enumerate(corpus_set):\n",
    "        w_to_i[w] = i\n",
    "        i_to_w[i] = w\n",
    "        freq = flat_corpus.count(w)   #**0.75\n",
    "        w_freq.append([i, freq])\n",
    "    return w_to_i, i_to_w, np.array(w_freq), num_words\n",
    "\n",
    "\n",
    "\n",
    "#SOME DECISIONS TO MAKE HERE: \n",
    "### 1st: Word window\n",
    "### 2nd: What to do when sentence is smaller than window size? Disregard? Fill with 0s? \n",
    "### If the above, fix vocab index for -UNK-\n",
    "### For now only keeping windows if sentences are large enough\n",
    "\n",
    "def make_word_windows(corpus, window_size):\n",
    "    windows = []\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        for i, word in enumerate(sentence):\n",
    "            curr_window = np.zeros(shape=[5])\n",
    "            \n",
    "            #Check if index allows large enough window sampling\n",
    "            if (i - WINDOW_SIZE > 0) and (i+ WINDOW_SIZE < len(sentence)):\n",
    "                word_index = w_to_i[word]\n",
    "                curr_window[0] = word_index\n",
    "                n=1\n",
    "                for j in range(i-WINDOW_SIZE, i+WINDOW_SIZE):\n",
    "                    context_index=w_to_i[sentence[j]]\n",
    "                    curr_window[n:] = context_index\n",
    "                    n+=1\n",
    "                windows.append(torch.LongTensor(curr_window))\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 2\n",
    "EMBEDDING_DIM = 128\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_corpus('wa/test.en')\n",
    "w_to_i, i_to_w, w_freq, num_words = make_dictionaries(corpus)\n",
    "data = make_word_windows(corpus, WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "class bayesian_skipgram(nn.Module):\n",
    "    def __init__(self, num_words, emb_dim):\n",
    "        super(bayesian_skipgram, self).__init__()\n",
    "\n",
    "        self.num_words = num_words\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        \n",
    "        self.R = nn.Embedding(num_words, emb_dim)\n",
    "        self.mu_prior = nn.Embedding(num_words, emb_dim)\n",
    "        self.sigma_prior  = nn.Embedding(num_words, emb_dim)\n",
    "        \n",
    "        self.M = nn.Linear(2*emb_dim, 2*emb_dim)\n",
    "        self.affine_lambda_mu = nn.Linear(2*emb_dim, emb_dim)\n",
    "        self.affine_lambda_sigma = nn.Linear(2*emb_dim, emb_dim)\n",
    "        self.affine_theta = nn.Linear(emb_dim, num_words)\n",
    "\n",
    "\n",
    "    def forward(self, word_idx, context_idx):\n",
    "        \n",
    "        R_w = self.R(word_idx) \n",
    "        R_w = R_w.unsqueeze(1)  \n",
    "        R_w = R_w.repeat(1, len(context_idx[1]), 1) \n",
    "        R_cj = self.R(context_idx)  \n",
    "        \n",
    "        RcRw = torch.cat((R_w, R_cj), dim=2)\n",
    "\n",
    "        h = nn.ReLU()(self.M(RcRw)) \n",
    "        h = torch.sum(h, dim=1)  \n",
    "\n",
    "\n",
    "        mu = self.affine_lambda_mu(h)\n",
    "        sigma = nn.Softplus()(self.affine_lambda_sigma(h))\n",
    "\n",
    "        # reparametrization trick\n",
    "        eps = MultivariateNormal(torch.zeros(self.emb_dim), torch.eye(self.emb_dim)).sample()\n",
    "        z = mu + torch.exp(sigma / 2.) * eps        \n",
    "       \n",
    "        #########################################\n",
    "    \n",
    "        affine_z = self.affine_theta(z)\n",
    "        \n",
    "        \n",
    "        f_i = nn.Softmax(dim=1)(affine_z)  \n",
    "        \n",
    "        mu_prior = self.mu_prior(word_idx)\n",
    "        sigma_prior = nn.Softplus()(self.sigma_prior(word_idx))\n",
    "            \n",
    "\n",
    "        ###################################################\n",
    "        \n",
    "        likelihood_term = []\n",
    "        KL_div_term = []\n",
    "        loss_term = []\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for i, contexts in enumerate(context_idx):\n",
    "            \n",
    "            #likelihood term\n",
    "            likelihood = 0\n",
    "            for idx in contexts:\n",
    "                likelihood += torch.log(f_i[i, idx])\n",
    "            likelihood_term.append(likelihood)\n",
    "            \n",
    "            #KL divergence term             \n",
    "            cov = torch.diagflat(sigma[i])\n",
    "            cov_prior = torch.diagflat(sigma_prior[i])\n",
    "            \n",
    "            KL = 0.5 * ( torch.log(torch.det(cov_prior) / torch.det(cov)) - self.emb_dim \n",
    "                                   + torch.trace(torch.matmul(torch.inverse(cov_prior), cov)) \n",
    "                       # + torch.matmul(torch.matmul((mu_prior[i] - mu[i]), torch.inverse(sigma_prior[i])), (mu_prior[i] - mu[i])))\n",
    "                        +  torch.matmul(torch.matmul((mu_prior[i] - mu[i]), torch.inverse(cov_prior)), (mu_prior[i] - mu[i])))\n",
    "            \n",
    "            KL_div_term.append(KL)\n",
    "            \n",
    "            \n",
    "            print(\"Losses: \", likelihood, KL, likelihood-KL)\n",
    "            \n",
    "            #total loss term\n",
    "            loss_term.append(likelihood - KL)\n",
    "            total_loss += (likelihood - KL)\n",
    "            \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z torch.Size([64, 128])\n",
      "Losses:  tensor(-29.1962) tensor(180.5776) tensor(-209.7738)\n",
      "Losses:  tensor(-28.9059) tensor(230.4201) tensor(-259.3260)\n",
      "Losses:  tensor(-30.4217) tensor(229.9865) tensor(-260.4082)\n",
      "Losses:  tensor(-31.7219) tensor(210.1728) tensor(-241.8947)\n",
      "Losses:  tensor(-29.6738) tensor(321.0730) tensor(-350.7467)\n",
      "Losses:  tensor(-32.7720) tensor(228.1464) tensor(-260.9185)\n",
      "Losses:  tensor(-32.5044) tensor(304.7249) tensor(-337.2293)\n",
      "Losses:  tensor(-29.8450) tensor(329.7832) tensor(-359.6282)\n",
      "Losses:  tensor(-27.9475) tensor(230.4641) tensor(-258.4116)\n",
      "Losses:  tensor(-29.3218) tensor(354.1818) tensor(-383.5035)\n",
      "Losses:  tensor(-31.3301) tensor(175.7688) tensor(-207.0989)\n",
      "Losses:  tensor(-32.2393) tensor(333.5015) tensor(-365.7408)\n",
      "Losses:  tensor(-31.5288) tensor(177.7138) tensor(-209.2426)\n",
      "Losses:  tensor(-31.4446) tensor(204.5961) tensor(-236.0407)\n",
      "Losses:  tensor(-31.1939) tensor(233.0972) tensor(-264.2910)\n",
      "Losses:  tensor(-31.9311) tensor(173.2609) tensor(-205.1921)\n",
      "Losses:  tensor(-31.7023) tensor(337.2581) tensor(-368.9605)\n",
      "Losses:  tensor(-31.2879) tensor(253.5594) tensor(-284.8473)\n",
      "Losses:  tensor(-30.0370) tensor(369.4779) tensor(-399.5149)\n",
      "Losses:  tensor(-31.0679) tensor(285.8749) tensor(-316.9427)\n",
      "Losses:  tensor(-30.3203) tensor(243.8231) tensor(-274.1435)\n",
      "Losses:  tensor(-34.2358) tensor(183.7769) tensor(-218.0127)\n",
      "Losses:  tensor(-30.5415) tensor(181.5728) tensor(-212.1144)\n",
      "Losses:  tensor(-32.7500) tensor(246.0917) tensor(-278.8416)\n",
      "Losses:  tensor(-30.7125) tensor(269.4175) tensor(-300.1300)\n",
      "Losses:  tensor(-29.9929) tensor(225.3893) tensor(-255.3822)\n",
      "Losses:  tensor(-33.7151) tensor(309.0775) tensor(-342.7925)\n",
      "Losses:  tensor(-31.3337) tensor(239.4677) tensor(-270.8014)\n",
      "Losses:  tensor(-30.4431) tensor(219.4116) tensor(-249.8548)\n",
      "Losses:  tensor(-29.5744) tensor(193.5802) tensor(-223.1546)\n",
      "Losses:  tensor(-26.7178) tensor(282.3692) tensor(-309.0870)\n",
      "Losses:  tensor(-34.1447) tensor(401.5399) tensor(-435.6845)\n",
      "Losses:  tensor(-33.8924) tensor(325.5052) tensor(-359.3976)\n",
      "Losses:  tensor(-33.1866) tensor(353.8674) tensor(-387.0540)\n",
      "Losses:  tensor(-34.4892) tensor(264.7627) tensor(-299.2520)\n",
      "Losses:  tensor(-29.0004) tensor(210.3980) tensor(-239.3984)\n",
      "Losses:  tensor(-31.4535) tensor(245.5183) tensor(-276.9718)\n",
      "Losses:  tensor(-31.5742) tensor(234.9685) tensor(-266.5427)\n",
      "Losses:  tensor(-31.2246) tensor(163.6112) tensor(-194.8358)\n",
      "Losses:  tensor(-31.7039) tensor(351.8455) tensor(-383.5494)\n",
      "Losses:  tensor(-29.5632) tensor(213.1872) tensor(-242.7504)\n",
      "Losses:  tensor(-30.9600) tensor(227.3917) tensor(-258.3517)\n",
      "Losses:  tensor(-31.8094) tensor(208.8070) tensor(-240.6164)\n",
      "Losses:  tensor(-27.1833) tensor(282.8320) tensor(-310.0153)\n",
      "Losses:  tensor(-27.9074) tensor(277.3718) tensor(-305.2792)\n",
      "Losses:  tensor(-32.1097) tensor(322.8091) tensor(-354.9188)\n",
      "Losses:  tensor(-32.9563) tensor(288.0471) tensor(-321.0034)\n",
      "Losses:  tensor(-35.0736) tensor(231.8153) tensor(-266.8889)\n",
      "Losses:  tensor(-31.9591) tensor(266.9581) tensor(-298.9173)\n",
      "Losses:  tensor(-30.1747) tensor(211.6594) tensor(-241.8340)\n",
      "Losses:  tensor(-31.3462) tensor(155.3984) tensor(-186.7445)\n",
      "Losses:  tensor(-34.4153) tensor(199.9421) tensor(-234.3574)\n",
      "Losses:  tensor(-32.0167) tensor(174.6299) tensor(-206.6466)\n",
      "Losses:  tensor(-29.2940) tensor(262.6678) tensor(-291.9618)\n",
      "Losses:  tensor(-30.2274) tensor(228.3635) tensor(-258.5909)\n",
      "Losses:  tensor(-30.7499) tensor(228.6908) tensor(-259.4406)\n",
      "Losses:  tensor(-33.2123) tensor(339.9751) tensor(-373.1874)\n",
      "Losses:  tensor(-29.5924) tensor(207.5013) tensor(-237.0936)\n",
      "Losses:  tensor(-30.3302) tensor(356.2341) tensor(-386.5643)\n",
      "Losses:  tensor(-32.6097) tensor(211.1017) tensor(-243.7115)\n",
      "Losses:  tensor(-31.4649) tensor(248.2519) tensor(-279.7167)\n",
      "Losses:  tensor(-31.0735) tensor(308.8246) tensor(-339.8980)\n",
      "Losses:  tensor(-32.4395) tensor(251.2262) tensor(-283.6656)\n",
      "Losses:  tensor(-29.5735) tensor(285.2947) tensor(-314.8683)\n",
      "-18293.736328125\n",
      "0.749821662902832\n",
      "z torch.Size([64, 128])\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n",
      "Losses:  tensor(nan.) tensor(nan.) tensor(nan.)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Lapack Error gesvd : 127 superdiagonals failed to converge. at c:\\programdata\\miniconda3\\conda-bld\\pytorch-cpu_1524541161962\\work\\aten\\src\\th\\generic/THTensorLapack.c:470",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-49373b353b7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBSG_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Lapack Error gesvd : 127 superdiagonals failed to converge. at c:\\programdata\\miniconda3\\conda-bld\\pytorch-cpu_1524541161962\\work\\aten\\src\\th\\generic/THTensorLapack.c:470"
     ]
    }
   ],
   "source": [
    "# TRAIN NETWORK\n",
    "%matplotlib inline\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(data, batch_size=64, shuffle=True)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "BSG_model = bayesian_skipgram(num_words, EMBEDDING_DIM)\n",
    "optimizer = torch.optim.Adam(BSG_model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    BSG_model.cuda()\n",
    "\n",
    "BSG_model.train()\n",
    "loss_progress = []\n",
    "iter_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, data_batch in enumerate(dataloader, 0):\n",
    "        main_word = data_batch[:,0]\n",
    "        context_word = data_batch[:,1:]\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_cuda:\n",
    "            loss = BSG_model.forward(main_word.cuda(), context_word.cuda())\n",
    "        else:\n",
    "            loss = BSG_model.forward(main_word, context_word)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 1000 == 0:\n",
    "            loss_progress.append(loss.item())  \n",
    "        if i % 25000 == 0:\n",
    "            print(loss_progress[-1])\n",
    "            print(time.time()-iter_time)\n",
    "            iter_time = time.time()\n",
    "\n",
    "\n",
    "print(\"total time taken:\", time.time()-start_time)\n",
    "plt.plot(loss_progress)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
