{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sindi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# read in data, remove punctuation and make all lower case\n",
    "def read_corpus(filename):\n",
    "    corpus = []\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    with open(filename) as f:\n",
    "        for s in f:\n",
    "            clean_s = s.translate(translator).lower()\n",
    "            clean_words = clean_s.split()\n",
    "            corpus.append(clean_words)\n",
    "    return corpus\n",
    "\n",
    "#Make data dictionaries and indexing \n",
    "def make_dictionaries(corpus):\n",
    "    # create one set of all unique words\n",
    "    flat_corpus = [w for s in corpus for w in s]\n",
    "    \n",
    "    corpus_words = set(flat_corpus)\n",
    "    print(\"Num unique words: \", len(corpus_words))\n",
    "    vocabulary = [v for v in corpus_words if v not in stop_words]\n",
    "    w_to_i = {}\n",
    "    i_to_w = {}\n",
    "    w_freq = {}\n",
    "    w_freq_all={}\n",
    "    #w_to_i['<unk>'] = 0\n",
    "    #i_to_w[0] = '<unk>'\n",
    "    \n",
    "    #V = len(vocabulary)\n",
    "    for i, w in enumerate(vocabulary):\n",
    "        w_to_i[w] = i + 1\n",
    "        i_to_w[i + 1] = w  \n",
    "        w_freq[w] = 0\n",
    "        \n",
    "    for i, w in enumerate(flat_corpus):\n",
    "        w_freq_all[w] = 0\n",
    "    \n",
    "    for i,w in enumerate(flat_corpus):\n",
    "        w_freq_all[w] += 1\n",
    "        \n",
    "    for i, w in enumerate(vocabulary):\n",
    "        w_freq[w] = w_freq_all[w]\n",
    "        \n",
    "        \n",
    "    w_to_i['<unk'] = 0\n",
    "    i_to_w[0] = '<unk>'\n",
    "        \n",
    "    print(\"Num filtered vocabulary: \", len(vocabulary))\n",
    "    \n",
    "    return w_to_i, i_to_w, w_freq, len(vocabulary)\n",
    "\n",
    "def make_word_windows(corpus, window_size):\n",
    "    windows = []\n",
    "    start = time.time()\n",
    "    for sentence in corpus:\n",
    "        for i, center_word in enumerate(sentence):           \n",
    "            if center_word in vocabulary:\n",
    "                curr_window = np.zeros(shape=[WINDOW_SIZE*2+1])\n",
    "                curr_window[0] = w_to_i[center_word]  #store center word first\n",
    "                \n",
    "                n = 1\n",
    "                #iterate over context words\n",
    "                for w in range(i - WINDOW_SIZE ,i+ WINDOW_SIZE ):\n",
    "                    #print(w, i)\n",
    "                    if w != i:        #avoid center word\n",
    "\n",
    "                        #check out of range\n",
    "                        if w > 0 and w < len(sentence):\n",
    "                            #print(w) \n",
    "                            #print(sentence[w])\n",
    "                            if sentence[w] in vocabulary:\n",
    "                                curr_window[n] = w_to_i[sentence[w]]\n",
    "                        n+=1\n",
    "\n",
    "                windows.append(torch.LongTensor(curr_window))\n",
    "    print('Context windows took %d seconds' % int(time.time()-start))\n",
    "    return windows\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 4\n",
    "EMBEDDING_DIM = 128\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num unique words:  31602\n",
      "Num filtered vocabulary:  31453\n"
     ]
    }
   ],
   "source": [
    "corpus = read_corpus('hansards/training.en')\n",
    "w_to_i, i_to_w, w_freq, V = make_dictionaries(corpus)\n",
    "vocabulary = w_to_i.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = make_word_windows(corpus, WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('w2i_bayesian.pkl', 'wb') as f:\n",
    "    pickle.dump(w_to_i, f)\n",
    "    \n",
    "with open('i2w_bayesian.pkl', 'wb') as f:\n",
    "    pickle.dump(i_to_w, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import distributions\n",
    "\n",
    "class bayesian_skipgram(nn.Module):\n",
    "    def __init__(self, num_words, emb_dim):\n",
    "        super(bayesian_skipgram, self).__init__()\n",
    "\n",
    "        self.num_words = num_words\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.R = nn.Embedding(num_words, emb_dim)\n",
    "        self.mu_prior = nn.Embedding(num_words, emb_dim)\n",
    "        self.sigma_prior  = nn.Embedding(num_words, emb_dim)\n",
    "        \n",
    "        self.M = nn.Linear(2*emb_dim, 2*emb_dim)\n",
    "        self.affine_lambda_mu = nn.Linear(2*emb_dim, emb_dim)\n",
    "        self.affine_lambda_sigma = nn.Linear(2*emb_dim, emb_dim)\n",
    "        self.affine_theta = nn.Linear(emb_dim, num_words)\n",
    "        \n",
    "\n",
    "    def forward(self, word_idx, context_idx):\n",
    "        \n",
    "        batch_size = word_idx.shape[0]\n",
    "        n_context = len(context_idx[0])\n",
    "        \n",
    "        # ********** Encoder ************\n",
    "        R_w = self.R(word_idx) \n",
    "        R_w = R_w.view(batch_size, 1, self.emb_dim)\n",
    "        R_w = R_w.repeat(1, n_context, 1) \n",
    "        \n",
    "\n",
    "        R_cj = self.R(context_idx)\n",
    "        \n",
    "        \n",
    "        RcRw = torch.cat((R_w, R_cj), dim=2)\n",
    "    \n",
    "        h = nn.ReLU()(self.M(RcRw)) \n",
    "        h = torch.sum(h, dim=1)   \n",
    "\n",
    "        mu = self.affine_lambda_mu(h)\n",
    "        sigma = nn.functional.softplus(self.affine_lambda_sigma(h))\n",
    "\n",
    "        # reparametrization trick\n",
    "        eps = distributions.MultivariateNormal(torch.zeros(self.emb_dim), torch.eye(self.emb_dim)).sample()\n",
    "        z = mu + sigma * eps    \n",
    "               \n",
    "            \n",
    "        # ********* Decoder ***********\n",
    "    \n",
    "        affine_categ = self.affine_theta(z)\n",
    "        f_i = nn.functional.softmax(affine_categ, dim=1)    \n",
    "                            \n",
    "        mu_prior = self.mu_prior(word_idx)\n",
    "        sigma_prior = nn.functional.softplus(self.sigma_prior(word_idx))\n",
    "            \n",
    "        # ********** Loss ************\n",
    "        \n",
    "        likelihood_terms = torch.zeros(batch_size)\n",
    "        KL_div_terms = torch.zeros(batch_size)\n",
    "        \n",
    "        for i, contexts in enumerate(context_idx):  \n",
    "            likelihood = 0\n",
    "            for idx in contexts:\n",
    "                likelihood += torch.log(f_i[i, idx] +1e-8)\n",
    "            likelihood_terms[i] = likelihood\n",
    "            \n",
    "            KL =  self.KL_div(mu_prior[i], sigma_prior[i],  mu[i],  sigma[i] )\n",
    "            KL_div_terms[i] = KL\n",
    "            \n",
    "          \n",
    "        total_loss = torch.mean(KL_div_terms) - torch.mean(likelihood_terms)\n",
    "             \n",
    "        return total_loss\n",
    "    \n",
    "    def KL_div(self,  mu_p, sigma_p, mu, sigma):\n",
    "        div = torch.log(sigma_p + 1e-8) - torch.log(sigma+1e-8) + (sigma**2 + (mu - mu_p)**2) / (2*sigma_p**2) - 0.5\n",
    "        return div.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN NETWORK\n",
    "%matplotlib inline\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "BSG_model = bayesian_skipgram(V, EMBEDDING_DIM)\n",
    "\n",
    "if use_cuda:\n",
    "    BSG_model.cuda()\n",
    "optimizer = torch.optim.Adam(BSG_model.parameters(), lr=0.005)\n",
    "BSG_model.train()\n",
    "loss_progress = []\n",
    "iter_time = time.time()\n",
    "\n",
    "dataloader = DataLoader(data, batch_size=16, shuffle=True)\n",
    "\n",
    "print(\"N_batches\", len(dataloader))\n",
    "\n",
    "for epoch in range(1):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch_start = time.time()\n",
    "        main_word = batch[:,0]\n",
    "        context_word = batch[:,1:]\n",
    "\n",
    "        #print(\"Main word:,\", main_word.shape, context_word.shape)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if use_cuda:\n",
    "            loss = BSG_model.forward(main_word.cuda(), context_word.cuda(), use_cuda=True)\n",
    "        else:\n",
    "            loss = BSG_model.forward(main_word, context_word)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_end = time.time() - batch_start\n",
    "        if i % 10 == 0:\n",
    "            print(\"epoch, batch \", epoch, i)\n",
    "            loss_progress.append(loss.item())  \n",
    "            print(time.time()-iter_time)\n",
    "            print(loss)\n",
    "            iter_time = time.time()\n",
    "print(\"total time taken:\", time.time()-start_time)\n",
    "plt.plot(loss_progress)\n",
    "plt.savefig('plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(BSG_model, 'bayesian_skipgram.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
