{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# read in data, remove punctuation and make all lower case\n",
    "def read_words(filename):\n",
    "    words = []\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    with open(filename) as f:\n",
    "        for s in f:\n",
    "            clean_s = s.translate(translator).lower()\n",
    "            words.append(clean_s.split())\n",
    "    return words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict that returns the index of onehot encoding for a word (and other way around)\n",
    "# also create a frequency dict + set size, usuable for negative sampling\n",
    "import numpy as np\n",
    "\n",
    "def get_onehot_dicts(corpus):\n",
    "    # create one set of all unique words\n",
    "    flat_corpus = [w for s in corpus for w in s]\n",
    "    corpus_set = set(flat_corpus)\n",
    "    w_to_i = {}\n",
    "    i_to_w = {}\n",
    "    w_freq = []\n",
    "    num_words = len(corpus_set)\n",
    "    for i, w in enumerate(corpus_set):\n",
    "        w_to_i[w] = i\n",
    "        i_to_w[i] = w\n",
    "        freq = flat_corpus.count(w)**0.75\n",
    "        w_freq.append([i, freq])\n",
    "    return w_to_i, i_to_w, np.array(w_freq), num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_words('wa/test.en')\n",
    "w_to_i, i_to_w, w_freq, num_words = get_onehot_dicts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all positive samples\n",
    "import torch\n",
    "def get_pos_pairs(corpus, window_size):\n",
    "    pairs = [] \n",
    "    for s in corpus:\n",
    "        for i, word in enumerate(s):\n",
    "            word_index = w_to_i[word]\n",
    "            for j in range(i-window_size, i+window_size+1):\n",
    "                if j < 0 or j >= len(s) or j == i:\n",
    "                    continue\n",
    "                context = s[j]\n",
    "                context_index = w_to_i[context]\n",
    "                pairs.append([word_index, context_index])\n",
    "    return torch.LongTensor(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = get_pos_pairs(corpus, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_pairs(num_samples, w_freq, pos_pairs):\n",
    "    neg_dict = {} # pos-pair : neg-pairs\n",
    "    total = w_freq[:,1].sum()\n",
    "    prob = [freq/total for freq in w_freq[:, 1]]\n",
    "    for pair in pos_pairs:\n",
    "        word = pair[0].item()\n",
    "        # get negative context words based on their frequencies\n",
    "        neg_contexts = np.random.choice(w_freq[:,0], p=prob, size=num_samples) \n",
    "        neg_dict[tuple(pair.numpy())] = torch.LongTensor(neg_contexts)\n",
    "    return neg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_dict = get_neg_pairs(5, w_freq, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dimension):\n",
    "        super(skipgram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dimension = emb_dimension\n",
    "        # start with random embeddings\n",
    "        self.W_embeddings = torch.randn((vocab_size, emb_dimension), requires_grad=True)\n",
    "        self.C_embeddings = torch.randn((vocab_size, emb_dimension), requires_grad=True)\n",
    "        \n",
    "    def forward(self, word, pos_context, neg_contexts):\n",
    "        # word, pos_context and neg_contexts are integers, so can just pick that row from W and C matrices\n",
    "        word_embed = self.W_embeddings[word]\n",
    "        pos_embed = self.C_embeddings[pos_context]\n",
    "        neg_embeds = self.C_embeddings.index_select(0, torch.LongTensor(neg_contexts))\n",
    "        \n",
    "        pos_similarity = torch.matmul(word_embed, pos_embed).squeeze()\n",
    "        pos_logsig = nn.functional.logsigmoid(pos_similarity)\n",
    "        pos_score = pos_logsig\n",
    "        \n",
    "        neg_similarity = torch.matmul(word_embed, neg_embeds.transpose(0,1)).squeeze()\n",
    "        neg_logsig = nn.functional.logsigmoid(-1 * neg_similarity)\n",
    "        neg_score = sum(neg_logsig)\n",
    "        \n",
    "        loss = -(pos_score+neg_score)\n",
    "        \n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49652\n"
     ]
    }
   ],
   "source": [
    "print(len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration  10000  is:  26.768859375\n",
      "this took  30.541802883148193  seconds\n",
      "loss at iteration  20000  is:  21.4815078125\n",
      "this took  30.86644697189331  seconds\n",
      "loss at iteration  30000  is:  19.5975296875\n",
      "this took  30.806231021881104  seconds\n",
      "loss at iteration  40000  is:  18.66006875\n",
      "this took  30.609694957733154  seconds\n",
      "loss at iteration  59652  is:  27.0302\n",
      "this took  60.07502889633179  seconds\n",
      "loss at iteration  69652  is:  10.70293984375\n",
      "this took  30.82400107383728  seconds\n",
      "loss at iteration  79652  is:  10.6948125\n",
      "this took  30.625030994415283  seconds\n",
      "loss at iteration  89652  is:  10.64741171875\n",
      "this took  30.868729829788208  seconds\n",
      "loss at iteration  109304  is:  16.407815625\n",
      "this took  60.18449592590332  seconds\n",
      "loss at iteration  119304  is:  6.84880078125\n",
      "this took  30.83335304260254  seconds\n",
      "loss at iteration  129304  is:  7.0220234375\n",
      "this took  30.854877948760986  seconds\n",
      "loss at iteration  139304  is:  6.966021875\n",
      "this took  30.714722633361816  seconds\n",
      "loss at iteration  158956  is:  11.1216203125\n",
      "this took  60.04704809188843  seconds\n",
      "loss at iteration  168956  is:  4.79339609375\n",
      "this took  30.77810287475586  seconds\n",
      "loss at iteration  178956  is:  4.96654296875\n",
      "this took  30.70874786376953  seconds\n",
      "loss at iteration  188956  is:  4.875822265625\n",
      "this took  30.739730834960938  seconds\n",
      "loss at iteration  208608  is:  8.06586484375\n",
      "this took  60.11149001121521  seconds\n",
      "loss at iteration  218608  is:  3.593328515625\n",
      "this took  30.877560138702393  seconds\n",
      "loss at iteration  228608  is:  3.715435546875\n",
      "this took  30.644754886627197  seconds\n",
      "loss at iteration  238608  is:  3.610569140625\n",
      "this took  30.58759617805481  seconds\n",
      "total time taken: 761.910306930542\n"
     ]
    }
   ],
   "source": [
    "# train skipgram\n",
    "import time\n",
    "start_time = time.time()\n",
    "iter_time = time.time()\n",
    "sg_model = skipgram(num_words, 200)\n",
    "optimizer = torch.optim.SGD([sg_model.W_embeddings, sg_model.C_embeddings], lr=0.01)\n",
    "\n",
    "sg_model.train()\n",
    "loss_sum = 0\n",
    "for epoch in range(5):\n",
    "    for i, pos_pair in enumerate(pairs):\n",
    "        neg_pairs = neg_dict[tuple(pos_pair.numpy())]\n",
    "        optimizer.zero_grad()\n",
    "        loss = sg_model.forward(pos_pair[0], pos_pair[1], neg_pairs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss\n",
    "        if i % 10000 == 0 and i!=0:\n",
    "            print(\"loss at iteration \", i+len(pairs)*epoch, \" is: \", loss_sum.item()/10000)\n",
    "            print(\"this took \", time.time()-iter_time, \" seconds\")\n",
    "            iter_time = time.time()\n",
    "            loss_sum = 0\n",
    "            \n",
    "print(\"total time taken:\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
